
```{r, echo = F, cache = F}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
options(width = 100)
```

# 小世界与大世界

不久前，The Oatmeal制作了一张[关于克里斯托弗-哥伦布的信息图表](http://theoatmeal.com/comics/columbus_day)。我不是历史学家，不能保证它的准确性，所以请自行决定。

McElreath这样描述本章的主旨：

> 在本章中，您将开始建立贝叶斯模型。贝叶斯模型从证据中学习的方法可以说是小世界中的最佳方法。当它们的假设逼近现实时，它们在大世界中的表现也很好。但是，大世界的表现必须通过证明而不是逻辑推导来实现。[@mcelreathStatisticalRethinkingBayesian2020, p. 20]

确实如此。

## 分叉数据花园

Gelman和Loken[-@gelmanGardenForkingPaths2013]写了一篇[名字相似的伟大论文](https://stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf)和题目。本节以及Gelman和Loken的论文中的标题起源于[Jorge Luis Borges](https://en.wikipedia.org/wiki/Jorge_Luis_Borges)的短篇小说[-@borgesjlJardinSenderosQue1941]，*岔路花园*。您可以在[这里](https://archive.org/stream/TheGardenOfForkingPathsJorgeLuisBorges1941/The-Garden-of-Forking-Paths-Jorge-Luis-Borges-1941_djvu.txt)或[这里](https://genius.com/Jorge-luis-borges-the-garden-of-forking-paths-annotated)找到短篇小说原文。这里有一个片段：

> 在所有的虚构作品中，每当一个人面对几种选择时，他会选择其中的一种而放弃其他的选择；而在Ts'ui Pên的小说中，他会同时选择所有的选择。通过这种方式，他创造了多种多样的未来，多种多样的时代，而这些时代本身也在增殖和分叉。

我们在数据分析中所做的选择也是以这种方式增殖和分叉的。

### 计算可能性。

在整个项目中，我们将广泛使用[**tidyverse**](https://www.tidyverse.org)中的软件包进行数据处理和绘图。

```{r, warning = F, message = F}
library(tidyverse)
```

如果你是**tidyverse**风格语法的新手，最奇怪的组件可能是管道（即`%>%`）。我不会在这个项目中解释`%>%`，但你可以在[这个简短的片段](https://youtu.be/9yjhxvu-pDg)中了解更多，从[Wickham演讲的第21:25分钟](https://youtu.be/K-ss_ag2k9E?t=1285)左右开始，或者在Grolemund和Wickham的[-@grolemundDataScience2017]*R for data science*中的[5.6.1节](https://r4ds.had.co.nz/transform.html#combining-multiple-operations-with-the-pipe)中了解更多。实际上，*R4DS*第5章的所有内容对于新的**R**和新的**tidyverse**用户来说都非常棒。而**R4DS*第3章则很好地介绍了如何使用**ggplot2**绘图[@R-ggplot2；@wickhamGgplot2ElegantGraphics2016]。

除了管道之外，另一个需要注意的重要东西是[tibbles](https://tibble.tidyverse.org) [@R-tibble]。就我们的目的而言，将tibble视为具有两个维度（由行和列定义）的数据对象。重要的是，tibble只是[数据帧](https://bookdown.org/rdpeng/rprogdatascience/r-nuts-and-bolts.html#data-frames)的特殊类型。因此，每当我们谈论数据帧时，我们通常都在谈论tibbles。有关该主题的更多信息，请参阅[*R4SD*，第10章](https://r4ds.had.co.nz/tibbles.html)。

如果我们愿意将弹珠编码为0="白色" 1="蓝色"，那么我们可以将可能性数据安排在如下的tibble中。

```{r, warning = F, message = F}
d <-
  tibble(p1 = 0,
         p2 = rep(1:0, times = c(1, 3)),
         p3 = rep(1:0, times = c(2, 2)),
         p4 = rep(1:0, times = c(3, 1)),
         p5 = 1)

head(d)
```

您可以在图表中描述可能性数据。

```{r, fig.width = 1.25, fig.height = 1.1}
d %>% 
  set_names(1:5) %>% 
  mutate(x = 1:4) %>% 
  pivot_longer(-x, names_to = "possibility") %>% 
  mutate(value = value %>% as.character()) %>% 
  
  ggplot(aes(x = x, y = possibility, fill = value)) +
  geom_point(shape = 21, size = 5) +
  scale_fill_manual(values = c("white", "navy")) +
  scale_x_discrete(NULL, breaks = NULL) +
  theme(legend.position = "none")
```

顺便提一下，查看Suzan Baert的博文[*数据处理第二部分：将列转换成正确的形状*](https://suzan.rbind.io/2018/02/dplyr-tutorial-2/)，其中对`dplyr::mutate()`和`tidyr::gather()`进行了广泛的讨论。`tidyr::pivot_longer()` 函数是 `gather()` 的更新变体，我们将在整个项目中广泛使用。如果您是使用透视重塑数据的新手，请查看小节 [此处](https://tidyr.tidyverse.org/reference/pivot_longer.html) 和 [此处](https://tidyr.tidyverse.org/articles/pivot.html) [@PivotDataWide2020; @Pivoting2020]。

以下是每次抽取弹珠的基本结构。

```{r, warning = F, message = F}
library(flextable)

tibble(draw    = 1:3,
       marbles = 4) %>% 
  mutate(possibilities = marbles ^ draw) %>% 
  flextable()
```

注意我们使用了**flextable**包[@R-flextable; @gohelUsingFlextable2023] 将输出格式化为漂亮的表格。我们将在本章中对此进行更多练习。

如果您稍加练习，就可以构建出接近图 2.2 所需的数据结构。

```{r}
(
  d <-
  tibble(position = c((1:4^1) / 4^0, 
                      (1:4^2) / 4^1, 
                      (1:4^3) / 4^2),
         draw     = rep(1:3, times = c(4^1, 4^2, 4^3)),
         fill     = rep(c("b", "w"), times = c(1, 3)) %>% 
           rep(., times = c(4^0 + 4^1 + 4^2)))
)
```

看到我的括号了吗？如果您在**R**中给一个对象赋值（例如，`dog <- 1`），然后点击return，[console](https://r4ds.had.co.nz/introduction.html#rstudio)中不会立即弹出任何内容。您必须实际执行`dog`，**R**才会返回`1`。但是，如果您将代码包在括号中（例如，`(dog <- 1)`），**R**将执行赋值并返回值，就像您执行了`dog`一样。

但我们离题了。下面是第一个图。

```{r, fig.width = 8, fig.height = 2}
d %>% 
  ggplot(aes(x = position, y = draw, fill = fill)) +
  geom_point(shape = 21, size = 3) +
  scale_fill_manual(values  = c("navy", "white")) +
  scale_y_continuous(breaks = 1:3) +
  theme(legend.position = "none",
        panel.grid.minor = element_blank())
```

在我看来，以适当的方式连接点的最简单方法是制作两个辅助tibbles。

```{r}
# these will connect the dots from the first and second draws
(
  lines_1 <-
  tibble(x    = rep(1:4, each = 4),
         xend = ((1:4^2) / 4),
         y    = 1,
         yend = 2)
  )

# these will connect the dots from the second and third draws
(
  lines_2 <-
  tibble(x    = rep((1:4^2) / 4, each = 4),
         xend = (1:4^3) / (4^2),
         y    = 2,
         yend = 3)
  )
```

我们可以用两个`geom_segment()`函数在绘图中使用`lines_1`和`lines_2`数据。

```{r, fig.width = 8, fig.height = 2}
d %>% 
  ggplot(aes(x = position, y = draw)) +
  geom_segment(data = lines_1,
               aes(x = x, xend = xend,
                   y = y, yend = yend),
               linewidth = 1/3) +
  geom_segment(data = lines_2,
               aes(x = x, xend = xend,
                   y = y, yend = yend),
               linewidth = 1/3) +
  geom_point(aes(fill = fill),
             shape = 21, size = 3) +
  scale_fill_manual(values  = c("navy", "white")) +
  scale_y_continuous(breaks = 1:3) +
  theme(legend.position = "none",
        panel.grid.minor = element_blank())
```

我们已经生成了 `位置`（即$x$轴）的值，可以说它们都向右对齐。但是我们希望将它们居中。对于 `draw ==1`，我们需要从每个中减去0.5。对于 `draw == 2`，我们需要将缩放4倍比例，对于 `draw == 3`，我们需要将缩放比例再减去4。`ifelse()` 函数将用于此。

```{r}
d <-
  d %>% 
  mutate(denominator = ifelse(draw == 1, .5,
                              ifelse(draw == 2, .5 / 4,
                                     .5 / 4^2))) %>% 
  mutate(position = position - denominator)

d
```

We'll follow the same logic for the `lines_1` and `lines_2` data.

```{r}
(
  lines_1 <-
  lines_1 %>% 
  mutate(x    = x - 0.5,
         xend = xend - 0.5 / 4^1)
)

(
  lines_2 <-
  lines_2 %>% 
  mutate(x    = x - 0.5 / 4^1,
         xend = xend - 0.5 / 4^2)
)
```

Now the plot's looking closer.

```{r, fig.width = 8, fig.height = 2}
d %>% 
  ggplot(aes(x = position, y = draw)) +
  geom_segment(data = lines_1,
               aes(x = x, xend = xend,
                   y = y, yend = yend),
               linewidth = 1/3) +
  geom_segment(data = lines_2,
               aes(x = x, xend = xend,
                   y = y, yend = yend),
               linewidth = 1/3) +
  geom_point(aes(fill = fill),
             shape = 21, size = 3) +
  scale_fill_manual(values  = c("navy", "white")) +
  scale_y_continuous(breaks = 1:3) +
  theme(legend.position = "none",
        panel.grid.minor = element_blank())
```

最后一步，我们将使用`coord_polar()`来改变[坐标系](http://sape.inf.usi.ch/quick-reference/ggplot2/coord)，使绘图具有曼陀罗的感觉。

```{r, fig.width = 4, fig.height = 4}
d %>% 
  ggplot(aes(x = position, y = draw)) +
  geom_segment(data = lines_1,
               aes(x = x, xend = xend,
                   y = y, yend = yend),
               linewidth = 1/3) +
  geom_segment(data = lines_2,
               aes(x = x, xend = xend,
                   y = y, yend = yend),
               linewidth = 1/3) +
  geom_point(aes(fill = fill),
             shape = 21, size = 4) +
  scale_fill_manual(values = c("navy", "white")) +
  scale_x_continuous(NULL, limits = c(0, 4), breaks = NULL) +
  scale_y_continuous(NULL, limits = c(0.75, 3), breaks = NULL) +
  coord_polar() +
  theme(legend.position = "none",
        panel.grid = element_blank())
```

为了制作图2.3的版本，我们必须添加一个索引来告诉我们在每次选择之后哪些路径在逻辑上仍然有效。我们将这个索引称为 `remain`。

```{r, fig.width = 4, fig.height = 4}
lines_1 <-
  lines_1 %>% 
  mutate(remain = c(rep(0:1, times = c(1, 3)),
                    rep(0,   times = 4 * 3)))
lines_2 <-
  lines_2 %>% 
  mutate(remain = c(rep(0,   times = 4),
                    rep(1:0, times = c(1, 3)) %>% rep(., times = 3),
                    rep(0,   times = 12 * 4)))
d <-
  d %>% 
  mutate(remain = c(rep(1:0, times = c(1, 3)),
                    rep(0:1, times = c(1, 3)),
                    rep(0,   times = 4 * 4),
                    rep(1:0, times = c(1, 3)) %>% rep(., times = 3),
                    rep(0,   times = 12 * 4))) 
# finally, the plot:
d %>% 
  ggplot(aes(x = position, y = draw)) +
  geom_segment(data = lines_1,
               aes(x = x, xend = xend,
                   y = y, yend = yend,
                   alpha = remain %>% as.character()),
               linewidth = 1/3) +
  geom_segment(data = lines_2,
               aes(x = x, xend = xend,
                   y = y, yend = yend,
                   alpha = remain %>% as.character()),
               linewidth = 1/3) +
  geom_point(aes(fill = fill, alpha = remain %>% as.character()),
             shape = 21, size = 4) +
  # it's the alpha parameter that makes elements semitransparent
  scale_fill_manual(values = c("navy", "white")) +
  scale_alpha_manual(values = c(1/5, 1)) +
  scale_x_continuous(NULL, limits = c(0, 4), breaks = NULL) +
  scale_y_continuous(NULL, limits = c(0.75, 3), breaks = NULL) +
  coord_polar() +
  theme(legend.position = "none",
        panel.grid = element_blank())
```

让 "w"=一个白点，"b"=一个蓝点，我们可以在第23页的中间重新创建这样的表格。

```{r}
# if we make two custom functions, here, 
# it will simplify the `mutate()` code, below
n_blue  <- function(x) rowSums(x == "b")
n_white <- function(x) rowSums(x == "w")

# make the data
t <-
  tibble(d1 = rep(c("w", "b"), times = c(1, 4)),
         d2 = rep(c("w", "b"), times = c(2, 3)),
         d3 = rep(c("w", "b"), times = c(3, 2)),
         d4 = rep(c("w", "b"), times = c(4, 1))) %>% 
  mutate(blue1 = n_blue(.),
         white = n_white(.),
         blue2 = n_blue(.)) %>% 
  mutate(product = blue1 * white * blue2)

# format the table
t %>%
  transmute(conjecture = str_c("[", d1, " ", d2, " ", d3, " ", d4, "]"),
            `Ways to produce [w b w]` = str_c(blue1, " * ", white, " * ", blue2, " = ", product)) %>% 
  flextable() %>% 
  width(j = 1:2, width = c(1, 2)) %>% 
  align(align = "center", part = "all")
```

We'll need new data for Figure 2.4. Here's the initial primary data, `d`.

```{r}
d <-
  tibble(position = c((1:4^1) / 4^0, 
                      (1:4^2) / 4^1, 
                      (1:4^3) / 4^2),
         draw     = rep(1:3, times = c(4^1, 4^2, 4^3)))

(
  d <-
  d %>% 
  bind_rows(
    d, d
  ) %>% 
  # here are the fill colors
  mutate(fill = c(rep(c("w", "b"), times = c(1, 3)) %>% rep(., times = c(4^0 + 4^1 + 4^2)),
                  rep(c("w", "b"), each  = 2)       %>% rep(., times = c(4^0 + 4^1 + 4^2)),
                  rep(c("w", "b"), times = c(3, 1)) %>% rep(., times = c(4^0 + 4^1 + 4^2)))) %>% 
  # now we need to shift the positions over in accordance with draw, like before
  mutate(denominator = ifelse(draw == 1, .5,
                              ifelse(draw == 2, .5 / 4,
                                     .5 / 4^2))) %>% 
  mutate(position = position - denominator) %>% 
  # here we'll add an index for which pie wedge we're working with
  mutate(pie_index = rep(letters[1:3], each = n()/3)) %>% 
  # to get the position axis correct for pie_index == "b" or "c", we'll need to offset
  mutate(position = ifelse(pie_index == "a", position,
                           ifelse(pie_index == "b", position + 4,
                                  position + 4 * 2)))
)
```

`lines_1` 和 `lines_2` 都需要调整 `x` 和 `xend`。我们目前的方法是嵌套`ifelse()`。与其将多行的 `ifelse()` 代码复制粘贴到所有四行中，不如将其封装到一个紧凑的函数中，我们称之为`move_over()`。

```{r}
move_over <- function(position, index) {
  ifelse(
    index == "a", position,
    ifelse(
      index == "b", position + 4, position + 4 * 2
    )
  )
}
```

如果你是第一次自己制作**R**函数，可以参考*R4DS*的[Chapter 19](https://r4ds.had.co.nz/functions.html)或者*R programming for data science*的[Chapter 14](https://bookdown.org/rdpeng/rprogdatascience/functions.html)[@pengProgrammingDataScience2022]。

总之，现在我们将创建新的`lines_1`和`lines_2`数据，我们将使用`move_over()`将它们的`x`和`xend`位置调整到正确的位置。

```{r}
(
  lines_1 <-
  tibble(x    = rep(1:4, each = 4) %>% rep(., times = 3),
         xend = ((1:4^2) / 4)      %>% rep(., times = 3),
         y    = 1,
         yend = 2) %>% 
  mutate(x    = x - .5,
         xend = xend - .5 / 4^1) %>% 
  # here we'll add an index for which pie wedge we're working with
  mutate(pie_index = rep(letters[1:3], each = n()/3)) %>% 
  # to get the position axis correct for `pie_index == "b"` or `"c"`, we'll need to offset
  mutate(x    = move_over(position = x,    index = pie_index),
         xend = move_over(position = xend, index = pie_index))
)

(
  lines_2 <-
  tibble(x    = rep((1:4^2) / 4, each = 4) %>% rep(., times = 3),
         xend = (1:4^3 / 4^2)              %>% rep(., times = 3),
         y    = 2,
         yend = 3) %>% 
  mutate(x    = x - .5 / 4^1,
         xend = xend - .5 / 4^2) %>% 
  # here we'll add an index for which pie wedge we're working with
  mutate(pie_index = rep(letters[1:3], each = n()/3)) %>% 
  # to get the position axis correct for `pie_index == "b"` or `"c"`, we'll need to offset
  mutate(x    = move_over(position = x,    index = pie_index),
         xend = move_over(position = xend, index = pie_index))
)
```

在最后的数据处理步骤中，我们添加 `remain` 索引来帮助我们确定哪些部分需要半透明。我不知道有什么巧妙的方法可以做到这一点，所以这些都是蛮力计算的结果。

```{r}
d <- 
  d %>% 
  mutate(remain = c(# pie_index == "a"
                    rep(0:1, times = c(1, 3)),
                    rep(0,   times = 4),
                    rep(1:0, times = c(1, 3)) %>% rep(., times = 3),
                    rep(0,   times = 4 * 4),
                    rep(c(0, 1, 0), times = c(1, 3, 4 * 3)) %>% rep(., times = 3),
                    # pie_index == "b"
                    rep(0:1, each = 2),
                    rep(0,   times = 4 * 2),
                    rep(1:0, each = 2) %>% rep(., times = 2),
                    rep(0,   times = 4 * 4 * 2),
                    rep(c(0, 1, 0, 1, 0), times = c(2, 2, 2, 2, 8)) %>% rep(., times = 2),
                    # pie_index == "c",
                    rep(0:1, times = c(3, 1)),
                    rep(0,   times = 4 * 3),
                    rep(1:0, times = c(3, 1)), 
                    rep(0,   times = 4 * 4 * 3),
                    rep(0:1, times = c(3, 1)) %>% rep(., times = 3),
                    rep(0,   times = 4)
                    )
         )

lines_1 <-
  lines_1 %>% 
  mutate(remain = c(rep(0,   times = 4),
                    rep(1:0, times = c(1, 3)) %>% rep(., times = 3),
                    rep(0,   times = 4 * 2),
                    rep(1:0, each  = 2) %>% rep(., times = 2),
                    rep(0,   times = 4 * 3),
                    rep(1:0, times = c(3, 1))
                    )
         )

lines_2 <-
  lines_2 %>% 
  mutate(remain = c(rep(0,   times = 4 * 4),
                    rep(c(0, 1, 0), times = c(1, 3, 4 * 3)) %>% rep(., times = 3),
                    rep(0,   times = 4 * 8),
                    rep(c(0, 1, 0, 1, 0), times = c(2, 2, 2, 2, 8)) %>% rep(., times = 2),
                    rep(0,   times = 4 * 4 * 3),
                    rep(0:1, times = c(3, 1)) %>% rep(., times = 3),
                    rep(0,   times = 4)
                    )
         )
```

We're finally ready to plot our Figure 2.4.

```{r, fig.width = 7, fig.height = 7}
d %>% 
  ggplot(aes(x = position, y = draw)) +
  geom_vline(xintercept = c(0, 4, 8), color = "white", linewidth = 2/3) +
  geom_segment(data = lines_1,
               aes(x = x, xend = xend,
                   y = y, yend = yend,
                   alpha = remain %>% as.character()),
               linewidth = 1/3) +
  geom_segment(data = lines_2,
               aes(x = x, xend = xend,
                   y = y, yend = yend,
                   alpha = remain %>% as.character()),
               linewidth = 1/3) +
  geom_point(aes(fill = fill, size = draw, alpha = remain %>% as.character()),
             shape = 21) +
  scale_fill_manual(values = c("navy", "white")) +
  scale_size_continuous(range = c(3, 1.5)) +
  scale_alpha_manual(values = c(0.2, 1)) +
  scale_x_continuous(NULL, limits = c(0, 12), breaks = NULL) +
  scale_y_continuous(NULL, limits = c(0.75, 3.5), breaks = NULL) +
  coord_polar() +
  theme(legend.position = "none",
        panel.grid = element_blank())
```

#### 结合其他信息。

> 我们可能有关于每个猜想相对可信性的额外信息。这些信息可能来自于关于袋子里的东西是如何产生的知识。也可能来自以前的数据。无论来源如何，有一种方法可以将不同来源的信息结合起来，更新可信度，这将会很有帮助。幸运的是，有一个自然的解决方案： 只需将计数相乘即可。(p. 25)

下面是第25页中间表格的制作方法。

```{r}
# update t
t <-
  t %>% 
  mutate(nc = blue1 * product)

# format the table
t %>%
  transmute(Conjecture            = str_c("[", d1, " ", d2, " ", d3, " ", d4, "]"),
            `Ways to produce [b]` = blue1,
            `Prior counts`        = product,
            `New count`           = str_c(blue1, " * ", product, " = ", nc)) %>% 
  flextable() %>% 
  width(width = c(1, 1, 0.8, 1)) %>% 
  align(align = "center", part = "all") %>% 
  align(j = 4, align = "left", part = "all") %>% 
  valign(valign = "bottom", part = "header")
```

我们可以将第26页顶部的表格用以下代码生成。

```{r}
# update t
t <-
  t %>% 
  rename(pc = nc) %>% 
  mutate(fc = c(0, 3:0)) %>% 
  mutate(nc = pc * fc) 

# format the table
t %>% 
  transmute(Conjecture      = str_c("[", d1, " ", d2, " ", d3, " ", d4, "]"),
            `Prior count`   = pc,
            `Factory count` = fc,
            `New count`     = str_c(pc, " * ", fc, " = ", nc)) %>% 
  flextable() %>% 
  width(width = c(1, 1, 0.8, 1)) %>% 
  align(align = "center", part = "all") %>% 
  align(j = 4, align = "left", part = "all") %>% 
  valign(valign = "bottom", part = "header")
```

要了解更多关于`dplyr::select()`和`dplyr::rename()`的信息，请查看Baert详尽的博文，[*数据处理第一部分：从基础到高级的列选择方法*](https://suzan.rbind.io/2018/01/dplyr-tutorial-1/)。

#### 反思： 最初的无知。

> 当没有关于猜想的先前信息时，我们应该使用哪种假设？最常见的解决方法是在看到任何数据之前，为每个猜想的正确性分配相等数量的方法。这有时被称为**冷漠原则**： 当没有理由说一种猜想比另一种猜想更可信时，对所有猜想进行同等权衡。本书不使用也不认可 "无知 "先验。我们将在后面的章节中看到，模型的结构和科学背景总是提供信息，使我们能够做得比 "无知 "更好。(第26页，原文**强调**)

### 从计数到概率。

本小节的开头几句话非常重要："将这一策略视为坚持诚实无知的原则是有帮助的： *当我们不知道数据的起因时，可能以更多方式产生数据的潜在起因更可信*"（第26页，原文*强调*）。

我们可以将更新后的可信度定义为

<center>

plausibility of ![](pictures/theta_02.png) after seeing ![](pictures/data_02.png)

$\propto$

ways ![](pictures/theta_02.png) can produce ![](pictures/data_02.png)

$\times$

prior plausibility of ![](pictures/theta_02.png).

</center>

换言之、

<center>

plausibility of $p$ after $D_\text{new}$ $\propto$ ways $p$ can produce $D_\text{new} \times$ prior plausibility of $p$.

</center>

但由于我们必须将结果标准化才能将其转化为概率度量，因此完整方程为

$$\text{plausibility of } p \text{ after } D_\text{new} = \frac{\text{ ways } p \text{ can produce } D_\text{new} \times \text{ prior plausibility of } p}{\text{sum of the products}}.$$

您可以将第27页中间的表格做成这样。

```{r}
# update t
t %>% 
  rename(ways = product) %>% 
  mutate(p = blue1 / 4) %>% 
  mutate(pl = ways / sum(ways)) %>% 
  transmute(`Possible composition` = str_c("[", d1, " ", d2, " ", d3, " ", d4, "]"),
            p                      = p,
            `Ways to produce data` = ways,
            `Plausibility`         = pl) %>% 
  
  # format for the table
  flextable() %>% 
  width(width = c(1.8, 1, 1.2, 1)) %>% 
  align(align = "center", part = "all") %>% 
  valign(valign = "bottom", part = "header") %>% 
  italic(j = 2, part = "header")
```

我们刚刚计算了可信度，下面是McElreath的**R**代码2.1。

```{r}
ways <- c(0, 3, 8, 9, 0)

ways / sum(ways)
```

## Building a model

We might save our globe-tossing data in a tibble.

```{r}
(d <- tibble(toss = c("w", "l", "w", "w", "w", "l", "w", "l", "w")))
```

### 数据故事。

>贝叶斯数据分析通常意味着产生一个数据如何产生的故事。这个故事可能是*描述性*的，具体说明可用于预测观察结果的关联。它也可能是*因果性的，是关于某些事件如何产生其他事件的理论。通常情况下，任何您希望成为因果关系的故事也可能是描述性的。但是，许多描述性故事很难从因果关系上进行解释。但所有的数据故事都是完整的，因为它们足以指定模拟新数据的算法。(第28页，原文*强调*)

###贝叶斯更新

在这里，我们将把累积的试验次数`n_trials`和累积的成功次数`n_successes`（即`toss =="w"`）添加到数据中。

```{r}
(
  d <-
  d %>% 
  mutate(n_trials  = 1:9,
         n_success = cumsum(toss == "w"))
  )
```

Fair warning: We don't learn the skills for making Figure 2.5 until later in the chapter. So consider the data wrangling steps in this section as something of a preview.

```{r, fig.width = 6, fig.height = 5}
sequence_length <- 50

d %>% 
  expand_grid(p_water = seq(from = 0, to = 1, length.out = sequence_length)) %>% 
  group_by(p_water) %>% 
  # to learn more about lagging, go to:
  # https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/lag
  # https://dplyr.tidyverse.org/reference/lead-lag.html
  mutate(lagged_n_trials  = lag(n_trials, k = 1),
         lagged_n_success = lag(n_success, k = 1)) %>% 
  ungroup() %>% 
  mutate(prior      = ifelse(n_trials == 1, .5,
                             dbinom(x    = lagged_n_success, 
                                    size = lagged_n_trials, 
                                    prob = p_water)),
         likelihood = dbinom(x    = n_success, 
                             size = n_trials, 
                             prob = p_water),
         strip      = str_c("n = ", n_trials)) %>% 
  # the next three lines allow us to normalize the prior and the likelihood, 
  # putting them both in a probability metric 
  group_by(n_trials) %>% 
  mutate(prior      = prior / sum(prior),
         likelihood = likelihood / sum(likelihood)) %>%   
  
  # plot!
  ggplot(aes(x = p_water)) +
  geom_line(aes(y = prior), 
            linetype = 2) +
  geom_line(aes(y = likelihood)) +
  scale_x_continuous("proportion water", breaks = c(0, .5, 1)) +
  scale_y_continuous("plausibility", breaks = NULL) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ strip, scales = "free_y")
```

如果代码不清楚，虚线是归一化的先验密度。实线是归一化似然值。如果不进行归一化处理（即用密度除以密度之和），它们各自的高度将与文本中的高度不一致。此外，正是归一化使它们具有直接可比性。

要了解更多关于`dplyr::group_by()`和与之相反的`dplyr::ungroup()`的信息，请查阅[*R4DS*, Chapter 5](https://r4ds.had.co.nz/transform.html)。要了解`tidyr::expand_grid()`，请访问[这里](https://tidyr.tidyverse.org/reference/expand_grid.html)。

#### 反思： 样本量和可靠推断。

> 我们经常听到这样的说法：一个有用的统计估计需要最少的观测值。例如，人们普遍迷信在使用高斯分布之前需要 30 个观测值。为什么呢？在非贝叶斯统计推断中，通常通过方法在非常大样本量下的行为（即所谓的渐近行为）来证明其合理性。因此，在小样本量下的表现是值得怀疑的。
>
> 相反，贝叶斯估计对任何样本量都有效。这并不意味着更多的数据没有帮助--当然有帮助。相反，无论样本大小如何，估计值都有清晰有效的解释。但这种能力的代价是依赖于初始似然性，即先验。如果先验值不好，那么推断结果就会产生误导。在了解世界的过程中，没有免费的午餐。(第31页，原文*强调*)

#### 评估。

> 只要贝叶斯模型能够准确地描述真实的大世界，那么它的学习方式显然是最优的。这就是说，你的贝叶斯机器保证了在小世界中的完美推理。从相同的信息状态开始，没有其他使用可用信息的方法可以做得更好。
>
> 然而，不要对这一逻辑优点过于兴奋。计算可能会出错，因此必须对结果进行检查。如果模型和现实之间存在重大差异，那么就无法从逻辑上保证大世界的性能。即使两个世界确实匹配，任何特定的数据样本仍然可能具有误导性。(p. 31)

#### 反思： 通货紧缩统计。

> 贝叶斯推理可能是已知的最好的通用推理方法。然而，贝叶斯推理的功能远没有我们希望的那么强大。没有一种推理方法能够提供普遍的保证。应用数学的任何分支都无法不受限制地接触现实，因为数学不像质子那样是被发现的。相反，它是被发明出来的，就像铁锹一样。(p. 32)

这种立场与有时被称为*数学柏拉图主义*的观点不谋而合，我怀疑许多科学家和普通人都持有这种立场。欲知更多详情，请查阅*数学哲学中的柏拉图主义*[@linneboPlatonismPhilosophyMathematics2018]。

##模型的组成部分

我们可以将模型的组成部分概括为三点：

1. 似然函数： "每个猜想产生观察结果的方式的数量"；
2. 一个或多个参数："每个猜想产生整个数据的累计方式数"；
3. 先验： "每个猜想产生整个数据的累积方式"；3.先验值："每个数据的猜想原因的初始可信度"（第 32 页）。

### 变量。

> 变量是可以取不同值的符号。在科学语境中，变量包括我们希望推断的东西，如比例和比率，以及我们可能观察到的东西，即数据....。
>
> 未观察到的变量通常称为**参数**。(第32页，原文中**强调**)

#### 定义。

> 一旦我们列出了变量，我们就必须定义每一个变量。在定义每一个变量时，我们要建立一个将变量相互联系起来的模型。记住，我们的目标是计算出在假设条件下数据可能产生的所有方式。(p. 33)

#### 观察变量。

> 因此，我们可以使用数学函数来告诉我们正确的可信度。在传统统计学中，分配给观测变量的分布函数通常称为**似然**。(第33页，**强调**在原文中）

如果水的数量为$w$，陆地的数量为$l$，那么折腾地球数据的二项似然值可以表示为

$$\Pr (w, l| p) = \frac{(w + l)!}{w!l!} p^w (1 - p)^l.$$

正如McElreath所写，我们可以将其理解为 "*'水'W和'陆地'L的计数是二项式分布的，每次掷出*'水'的概率* $p$。(第33页，原文中*强调*）。鉴于概率为0.5，我们可以使用`dbinom()`函数来确定9次掷骰子中6次出水的可能性。

```{r}
dbinom(x = 6, size = 9, prob = .5)
```

McElreath建议我们改变`prob`的值。下面是在参数空间$[0, 1]$中改变值的方法。

```{r, fig.width = 3, fig.height = 2}
tibble(prob = seq(from = 0, to = 1, by = .01)) %>% 
  ggplot(aes(x = prob, y = dbinom(x = 6, size = 9, prob = prob))) +
  geom_line() +
  labs(x = "probability",
       y = "binomial likelihood") +
  theme(panel.grid = element_blank())
```

##### 过度思考： 名称和概率分布。

> `dbinom` 中的"`d`"代表*密度*。以这种方式命名的函数几乎总是有对应的以"`r`"开头的随机样本函数和以"`p`"开头的累积概率函数. 参见帮助 `?dbinom`。(第34页，*强调*为原文)

#### 非观察变量。

> 观察变量的分布一般都有自己的变量。在上面的二项式中，有$pv，即取样水的概率。由于$p$是不可观测的，我们通常称其为**参数。尽管我们无法观察到$p$，但仍需对其进行定义。(第34页，**强调**为原文)

#### 过度思考： 作为概率分布的先验

McElreath说："对于从$a$到$b$的均匀先验，区间内任何一点的概率都是$1 / (b - a)$"（第35页）。让我们试试看。为了简单起见，我们将保持 $a$ 不变，同时改变 $b$ 的值。

```{r}
tibble(a = 0,
       b = c(1, 1.5, 2, 3, 9)) %>% 
  mutate(prob = 1 / (b - a))
```

I like to verify things with plots.

```{r, fig.width = 8, fig.height = 2}
tibble(a = 0,
       b = c(1, 1.5, 2, 3, 9)) %>% 
  expand_grid(parameter_space = seq(from = 0, to = 9, length.out = 500)) %>% 
  mutate(prob = dunif(parameter_space, a, b),
         b    = str_c("b = ", b)) %>% 
  
  ggplot(aes(x = parameter_space, y = prob)) +
  geom_area() +
  scale_x_continuous(breaks = c(0, 1:3, 9)) +
  scale_y_continuous(breaks = c(0, 1/9, 1/3, 1/2, 2/3, 1),
                     labels = c("0", "1/9", "1/3", "1/2", "2/3", "1")) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank()) +
  facet_wrap(~ b, ncol = 5)
```

As we'll learn much later in the project, the $\operatorname{Uniform}(0, 1)$ distribution is special in that we can also express it as the beta distribution for which $\alpha = 1 \text{ and } \beta = 1$. E.g.,

```{r, fig.width = 2.5, fig.height = 2}
tibble(parameter_space = seq(from = 0, to = 1, length.out = 50)) %>% 
  mutate(prob = dbeta(parameter_space, 1, 1)) %>% 
  
  ggplot(aes(x = parameter_space, y = prob)) +
  geom_area() +
  scale_y_continuous("density", limits = c(0, 2)) +
  ggtitle(expression("This is beta"*(1*", "*1))) +
  theme(panel.grid = element_blank())
```

#### 反思： 数据还是参数？

> 将数据和参数视为完全不同的实体是很典型的。数据是测量过的，是已知的；参数是未知的，必须根据数据进行估计。有用的是，在贝叶斯框架中，数据和参数之间的区别并不那么根本。(p. 35)

关于这个主题的更多信息，请参阅McElreath的演讲[*不使用频率学派术语理解贝叶斯统计*](https://youtu.be/yakg94HyWdE)。

### 一个模型诞生了。

现在我们可以用二项似然中的参数来描述我们的观测变量$w$和$l$，我们的速记符号是

$$w \sim \operatorname{Binomial}(n, p),$$

where $n = w + l$. Our binomial likelihood contains a parameter for an unobserved variable, $p$. Parameters in Bayesian models are assigned priors and we can report our prior for $p$ as

$$p \sim \operatorname{Uniform}(0, 1),$$

which expresses the model assumption that the entire range of possible values for $p$, $[0, 1$, are equally plausible.

## 建立模型

> 对于数据、似然比、参数和先验的每一个独特组合，都有一个独特的后验分布。这个分布包含了不同参数值在数据和模型条件下的相对可信度。后验分布*的形式是以数据为条件的参数概率。(第36页，*强调*后加)

###贝叶斯定理。

我们已经知道$w$、$l$以及逻辑上必然的$n$的值。贝叶斯定理将允许我们在给定$w$和$l$的情况下确定$p$的各种值的可信度，我们可以将其正式表示为$\Pr(p | w, l)$。在第37页前面的一些方程的基础上，贝叶斯定理告诉我们

$$\Pr(p | w, l) = \frac{\Pr(w, l | p) \Pr(p )}{\Pr(w, l)}.$$

> 这就是贝叶斯定理。它说，考虑到数据，$p$的任何特定值的概率等于以$p$为条件的数据的相对可信度与$p$的先验可信度的乘积，再除以$\Pr(W, L)$，我称之为数据的*平均概率。(第37页，原文*强调*)

我们可以将其表达为

$$\text{Posterior} = \frac{\text{Probability of the data} \times \text{Prior}}{\text{Average probability of the data}}.$$

数据的平均概率通常被称作 "证据 "或 "平均似然"，我们将逐渐了解其含义。"关键的一课是后验与先验和数据概率的乘积成正比"（第 37 页）。图 2.6 将帮助我们理解这一点。以下是数据的准备步骤。

```{r}
sequence_length <- 1e3

d <-
  tibble(probability = seq(from = 0, to = 1, length.out = sequence_length)) %>% 
  expand_grid(row = c("flat", "stepped", "Laplace"))  %>% 
  arrange(row, probability) %>% 
  mutate(prior = ifelse(row == "flat", 1,
                        ifelse(row == "stepped", rep(0:1, each = sequence_length / 2),
                               exp(-abs(probability - 0.5) / .25) / ( 2 * 0.25))),
         likelihood = dbinom(x = 6, size = 9, prob = probability)) %>% 
  group_by(row) %>% 
  mutate(posterior = prior * likelihood / sum(prior * likelihood)) %>% 
  pivot_longer(prior:posterior)  %>% 
  ungroup() %>% 
  mutate(name = factor(name, levels = c("prior", "likelihood", "posterior")),
         row  = factor(row, levels = c("flat", "stepped", "Laplace")))
```

To learn more about `dplyr::arrange()`, check out [*R4DS*, Chapter 5.3](https://r4ds.had.co.nz/transform.html#arrange-rows-with-arrange).

为了避免对行进行不必要的分面标注，我们只需分别制作每一列的曲线图。然后，我们可以使用[Thomas Lin Pedersen](https://www.data-imaginist.com/)的[-@R-patchwork][**patchwork**包](https://patchwork.data-imaginist.com/)中优雅而强大的语法将它们组合起来。

```{r, fig.width = 6, fig.height = 5, warning = F, message = F}
p1 <-
  d %>%
  filter(row == "flat") %>% 
  ggplot(aes(x = probability, y = value)) +
  geom_line() +
  scale_x_continuous(NULL, breaks = NULL) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ name, scales = "free_y")

p2 <-
  d %>%
  filter(row == "stepped") %>% 
  ggplot(aes(x = probability, y = value)) +
  geom_line() +
  scale_x_continuous(NULL, breaks = NULL) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank(),
        strip.background = element_blank(),
        strip.text = element_blank()) +
  facet_wrap(~ name, scales = "free_y")

p3 <-
  d %>%
  filter(row == "Laplace") %>% 
  ggplot(aes(x = probability, y = value)) +
  geom_line() +
  scale_x_continuous(NULL, breaks = c(0, .5, 1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank(),
        strip.background = element_blank(),
        strip.text = element_blank()) +
  facet_wrap(~ name, scales = "free_y")

# combine
library(patchwork)
p1 / p2 / p3
```

I'm not sure if it's the same McElreath used in the text, but the formula I used for the triangle-shaped prior is the [Laplace distribution](https://rdrr.io/cran/rmutil/man/Laplace.html) with a location of 0.5 and a dispersion of 0.25.

Also, to learn all about `dplyr::filter()`, check out Baert's [*Data wrangling part 3: Basic and more advanced ways to filter rows*](https://suzan.rbind.io/2018/02/dplyr-tutorial-3/).

### Motors.

> Various numerical techniques are needed to approximate the mathematics that follows from the definition of Bayes' theorem. In this book, you'll meet three different conditioning engines, numerical techniques for computing posterior distributions:
>
> 1. Grid approximation
> 2. Quadratic approximation
> 3. Markov chain Monte Carlo (MCMC)
>
> There are many other engines, and new ones are being invented all the time. But the three you'll get to know here are common and widely useful. (p. 39)

`r emo::ji("warning")` In this translation of McElreath's text, we will get a little practice with grid approximation and the quadratic approximation. But since our aim is to practice with **brms**, we'll jump rather quickly into MCMC. This will be awkward at times because it will force us to contend with technical issues in earlier problems in the text than McElreath originally did. I'll do what I can to bridge the pedagogical gaps.

### 网格近似。

继续我们的折腾地球的例子、

> 在任何特定的参数值$p'$下, 计算后验概率是很简单的: 只需将$p'$的先验概率乘以$p'$下的可能性. 对网格中的每个值重复这一过程，就可以得到精确后验分布的近似值。这个过程被称为**网格近似**。(第39-40页，原文**强调**)

我们刚刚在最后一张图中使用了网格逼近法。为了得到平滑的线条，我们计算了概率空间上1000个均匀分布点的后验。这里我们将为图 2.7 中的 20.

```{r}
(
  d <-
    tibble(p_grid = seq(from = 0, to = 1, length.out = 20),      # define grid
           prior  = 1) %>%                                       # define prior
    mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %>%  # compute likelihood at each value in grid
    mutate(unstd_posterior = likelihood * prior) %>%             # compute product of likelihood and prior
    mutate(posterior = unstd_posterior / sum(unstd_posterior))   # standardize the posterior, so it sums to 1
)
```

Here's the code for the right panel of Figure 2.7.

```{r}
p1 <-
  d %>% 
  ggplot(aes(x = p_grid, y = posterior)) +
  geom_point() +
  geom_line() +
  labs(subtitle = "20 points",
       x = "probability of water",
       y = "posterior probability") +
  theme(panel.grid = element_blank())
```

Now here's the code for the left hand panel of Figure 2.7.

```{r}
p2 <-
  tibble(p_grid = seq(from = 0, to = 1, length.out = 5),
         prior  = 1) %>%
  mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %>%
  mutate(unstd_posterior = likelihood * prior) %>%
  mutate(posterior = unstd_posterior / sum(unstd_posterior)) %>% 
  
  ggplot(aes(x = p_grid, y = posterior)) +
  geom_point() +
  geom_line() +
  labs(subtitle = "5 points",
       x = "probability of water",
       y = "posterior probability") +
  theme(panel.grid = element_blank())
```

Here we combine them and plot!

```{r, fig.width = 6, fig.height = 3}
p2 + p1 + plot_annotation(title = "More grid points make for smoother approximations")
```

In his **R** code 2.5 box, McElreath encouraged us to redo those plots with the two new kinds of priors.

```{r, eval = F}
prior <- ifelse( p_grid < 0.5 , 0 , 1 ) 
prior <- exp( -5*abs( p_grid - 0.5 ) )
```

Here's a condensed way to make the four plots all at once.

```{r, fig.width = 6, fig.height = 5}
# make the data
tibble(n_points = c(5, 20)) %>% 
  mutate(p_grid = map(n_points, ~seq(from = 0, to = 1, length.out = .))) %>% 
  unnest(p_grid) %>% 
  expand_grid(priors = c("ifelse(p_grid < 0.5, 0, 1)", "exp(-5 * abs(p_grid - 0.5))")) %>% 
  mutate(prior = ifelse(priors == "ifelse(p_grid < 0.5, 0, 1)", 
                        ifelse(p_grid < 0.5, 0, 1),
                        exp(-5 * abs(p_grid - 0.5)))) %>% 
  mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %>% 
  mutate(posterior = likelihood * prior / sum(likelihood * prior)) %>% 
  mutate(n_points = str_c("# points = ", n_points),
         priors   = str_c("prior = ", priors)) %>% 
  
  # plot!
  ggplot(aes(x = p_grid, y = posterior)) +
  geom_line() +
  geom_point() +
  labs(x = "probability of water",
       y = "posterior probability") +
  theme(panel.grid = element_blank()) +
  facet_grid(n_points ~ priors, scales = "free")
```

#### 二次近似

> 在一般情况下，后验分布峰值附近的区域近似于高斯分布或 "正态分布"。这意味着后验分布可以用高斯分布来近似。高斯分布很方便，因为它可以完全用两个数字来描述：它的中心位置（均值）和扩散（方差）。
>
> 高斯近似之所以被称为 "*二次近似*"，是因为高斯分布的对数形成了抛物线。而抛物线是二次函数。因此，这种近似方法基本上是用抛物线来表示任何对数前值。(第42页，*着重号*为作者所加）

虽然McElreath在前半部分使用了二次函数近似值，但在本章之后我们就不会再使用它了。在这里，我们将使用`rethinking::quap()`函数对抛球数据进行二次逼近。

```{r, warning = F, message = F}
library(rethinking)

globe_qa <- quap(
  alist(
    W ~ dbinom(W + L, p),  # binomial likelihood 
    p ~ dunif(0, 1)        # uniform prior
  ), 
  data = list(W = 6, L = 3)
)

# display summary of quadratic approximation 
precis(globe_qa)
```

In preparation for Figure 2.8, here's the model with $n = 18$ and $n = 36$.

```{r}
globe_qa_18 <-
  quap(
    alist(
      w ~ dbinom(9 * 2, p),
      p ~ dunif(0, 1)
    ), data = list(w = 6 * 2))

globe_qa_36 <-
  quap(
    alist(
      w ~ dbinom(9 * 4, p),
      p ~ dunif(0, 1)
    ), data = list(w = 6 * 4))

precis(globe_qa_18)
precis(globe_qa_36)
```

Now make Figure 2.8.

```{r, fig.width = 8, fig.height = 2.75}
n_grid <- 100

# wrangle
tibble(w = c(6, 12, 24),
       n = c(9, 18, 36),
       s = c(.16, .11, .08)) %>% 
  expand_grid(p_grid = seq(from = 0, to = 1, length.out = n_grid)) %>% 
  mutate(prior = 1,
         m     = .67)  %>%
  mutate(likelihood = dbinom(w, size = n, prob = p_grid)) %>%
  mutate(unstd_grid_posterior = likelihood * prior,
         unstd_quad_posterior = dnorm(p_grid, m, s)) %>%
  group_by(w) %>% 
  mutate(grid_posterior = unstd_grid_posterior / sum(unstd_grid_posterior),
         quad_posterior = unstd_quad_posterior / sum(unstd_quad_posterior),
         n              = str_c("n = ", n)) %>% 
  mutate(n = factor(n, levels = c("n = 9", "n = 18", "n = 36"))) %>% 
  
  # plot
  ggplot(aes(x = p_grid)) +
  geom_line(aes(y = grid_posterior)) +
  geom_line(aes(y = quad_posterior),
            color = "grey50") +
  labs(x = "proportion water",
       y = "density") +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ n, scales = "free")
```

> 这种现象非常普遍，即二次近似值随着数据量的增加而提高。这也是许多经典统计程序对小样本感到紧张的原因之一： 这些统计过程使用的二次近似（或其他近似）只有在数据量为无限时才是安全的。通常情况下，这些近似值对于小于无限的数据显然是有用的。但是，随着样本量的增加，其改进率会因具体情况的不同而有很大差异。在某些模型中，即使有成千上万的样本，二次近似仍然很糟糕。(p. 44)

#### 反思： 最大似然估计

> 在均匀先验或大量数据的情况下，二次近似通常等价于**最大似然估计** (MLE)及其**标准误差**。MLE是一种非常常见的非贝叶斯参数估计。贝叶斯近似值与常见的非贝叶斯估计值之间的这种对应关系是福也是祸。说它是福，是因为它允许我们用贝叶斯方法重新解释各种已发表的非贝叶斯模型拟合。说它是祸，是因为最大似然估计有一些奇怪的缺点，而二次近似可以分担这些缺点。(第44页，**强调**，原文如此)

强调广义线性模型最大似然法的教科书比比皆是。如果这对您来说是新的，您想了解更多，也许可以参考Roback和Legler的[-@roback2021beyond][*超越多元线性回归： https://bookdown.org/roback/bookdown-BeyondMLR/), Agresti的 [-@agrestiFoundationsLinearGeneralized2015] [*Foundations of linear and generalized linear models*](https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034) 或者Dunn和Smyth的 [-@dunn2018generalized] [*Generalized linear models with examples in R*](https://link.springer.com/book/10.1007/978-1-4419-0118-7).

###马尔科夫链蒙特卡罗。

> 最流行的[网格逼近和二次逼近的替代方法]是**马尔科夫链蒙特卡罗**（MCMC），这是一个能够处理高度复杂模型的条件引擎系列。可以说，MCMC是20世纪90年代贝叶斯数据分析兴起的主要原因。虽然MCMC的历史比20世纪90年代还要悠久，但可负担得起的计算机能力却并非如此，因此我们还必须感谢工程师们。~~在本书的稍后部分（第9章）~~~，您将看到MCMC模型拟合的简单而精确的例子，旨在帮助您理解这项技术。(第45页，原文**强调**)

**brms**包使用MCMC的一个版本来拟合贝叶斯模型。由于本项目的主要目标之一是突出**brms**，我们不妨拟合一个模型。这似乎是一个合适的子节。首先，我们需要加载软件包。

```{r, warning = F, message = F}
library(brms)
```

If you haven't already installed **brms**, you can find instructions on how to do so [here](https://github.com/paul-buerkner/brms#how-do-i-install-brms).

Here we re-fit the last model from above, the one for which $w = 24$ and $n = 36$.

```{r b2.1}
b2.1 <-
  brm(data = list(w = 24), 
      family = binomial(link = "identity"),
      w | trials(36) ~ 0 + Intercept,
      prior(beta(1, 1), class = b, lb = 0, ub = 1),
      seed = 2,
      file = "fits/b02.01")
```

从**brms**输出的模型如下所示。

```{r}
print(b2.1)
```

我们将在[第 4 章][地心模型]中对该输出结果进行说明。现在，请关注 "截距 "线。我们将在第 4 章中学习到，没有预测因子的典型回归模型的截距与平均值相同。在使用二项似然法的模型的特殊情况下，均值是给定试验中出现1的概率，$\theta$。
 
另外，使用**brms**，有很多方法来总结模型的结果。`brms::posterior_summary()` 函数类似于`rethinking::precis()`。然而，我们需要使用`round()`将输出结果减少到合理的小数位数。

```{r}
posterior_summary(b2.1) %>% 
  round(digits = 2)
```

The `b_Intercept` row is the probability. Don't worry about the second line, for now. We'll cover the details of **brms** model fitting in later chapters. To finish up, why not plot the results of our model and compare them with those from `rethinking::quap()`, above?

```{r, fig.width = 3, fig.height = 2.75}
as_draws_df(b2.1) %>% 
  mutate(n = "n = 36") %>%
  
  ggplot(aes(x = b_Intercept)) +
  geom_density(fill = "black") +
  scale_x_continuous("proportion water", limits = c(0, 1)) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ n)
```

如果您仍然感到困惑，没关系。这只是一个预览。在[第4章][地心模型]中, 我们将开始用**brms**拟合模型, 在[第11章][God Spiked the Integers]中, 我们将学习很多关于二项似然回归的知识.

## Session信息 {-}

```{r}
sessionInfo()
```

```{r, echo = F, warning = F, message = F}
rm(d, lines_1, lines_2, n_blue, n_white, t, move_over, ways, sequence_length, p1, p2, p3, globe_qa, globe_qa_18, globe_qa_36, n_grid, b2.1)
```

```{r, echo = F, warning = F, message = F, results = "hide"}
# pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```

